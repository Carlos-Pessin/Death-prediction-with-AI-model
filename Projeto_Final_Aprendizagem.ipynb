{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carlos-Pessin/Death-prediction-with-AI-model/blob/main/Projeto_Final_Aprendizagem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dwTW9W5Fqz_",
        "outputId": "2ff9b288-9a50-430b-9af8-461c326c8343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kGiJgtVx3Zw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Bibs/')\n",
        "import main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lEYROJAFz2E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W25jHUNOhafO"
      },
      "outputs": [],
      "source": [
        "#%% IMPORT DOS DADOS\n",
        "X = pd.read_excel('/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/training_v3.xlsx', sheet_name='X')\n",
        "y = pd.read_excel('/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/training_v3.xlsx', sheet_name='y')\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "X = np.transpose(X)\n",
        "y = y[:,0]\n",
        "(L,N) = np.shape(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE OUTLIERS GERAL\n",
        "N_excl = 0\n",
        "X_o = X\n",
        "y_o = y\n",
        "for i in range(L):\n",
        "    outliers = np.array(main.t3_remoutliers(X_o[i,:], 15))\n",
        "    N_excl = N_excl + len(outliers)\n",
        "    if np.size(outliers)>0:\n",
        "        X_o = np.delete(X_o,outliers,axis=1)\n",
        "        y_o = np.delete(y_o,outliers)\n",
        "\n",
        "\n",
        "# SEPARA CLASSES\n",
        "y1_o = y_o>0\n",
        "X1_o = X_o[:,y1_o]\n",
        "X0_o = np.delete(X_o,y1_o,axis=1)\n",
        "y0_o = np.delete(y_o,y1_o)\n",
        "y1_o = y_o[y1_o]\n",
        "\n",
        "\n",
        "# SEPARA TREINO E TESTE\n",
        "X0_train, X0_test, y0_train, y0_test = train_test_split(np.transpose(X0_o), y0_o, test_size=0.1, random_state=0)\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(np.transpose(X1_o), y1_o, test_size=0.1, random_state=0)\n",
        "X_train = np.transpose(np.vstack((X1_train,X0_train)))\n",
        "y_train = np.concatenate((y1_train,y0_train))\n",
        "X_test = np.transpose(np.vstack((X1_test,X0_test)))\n",
        "y_test = np.concatenate((y1_test,y0_test))\n",
        "\n",
        "\n",
        "# NORMALIZACAO\n",
        "X_train_n = (X_train - np.mean(X_train))/np.std(X_train,ddof=1)\n",
        "X_test_n = (X_test - np.mean(X_train))/np.std(X_train,ddof=1)\n",
        "\n",
        "# SEPARA CLASSES\n",
        "y1_train_n = y_train>0\n",
        "X1_train_n = X_train_n[:,y1_train_n]\n",
        "X0_train_n = np.delete(X_train_n,y1_train_n,axis=1)\n",
        "y0_train_n = np.delete(y_train,y1_train_n)\n",
        "y1_train_n = y_train[y1_train_n]\n",
        "\n",
        "X_train_n=np.transpose(X_train_n)"
      ],
      "metadata": {
        "id": "DKMAGcjg-UCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inicializa metricas medias\n",
        "SVM_sig_trainmed = {}\n",
        "SVM_sig_valmed = {}\n",
        "\n",
        "SVM_sig_trainmed['acc_train'] = 0\n",
        "SVM_sig_trainmed['confMatrix_train'] = np.zeros((2,2))\n",
        "SVM_sig_trainmed['TP_train'] = 0\n",
        "SVM_sig_trainmed['FN_train'] = 0\n",
        "SVM_sig_trainmed['TN_train'] = 0\n",
        "SVM_sig_trainmed['FP_train'] = 0\n",
        "SVM_sig_trainmed['Sensibilidade_train'] = 0\n",
        "SVM_sig_trainmed['Especificidade_train'] = 0\n",
        "SVM_sig_trainmed['Precision_train'] = 0\n",
        "SVM_sig_trainmed['auc_train'] = 0\n",
        "SVM_sig_valmed['acc_val'] = 0\n",
        "SVM_sig_valmed['confMatrix_val'] = np.zeros((2,2))\n",
        "SVM_sig_valmed['TP_val'] = 0\n",
        "SVM_sig_valmed['FN_val'] = 0\n",
        "SVM_sig_valmed['TN_val'] = 0\n",
        "SVM_sig_valmed['FP_val'] = 0\n",
        "SVM_sig_valmed['Sensibilidade_val'] = 0\n",
        "SVM_sig_valmed['Especificidade_val'] = 0\n",
        "SVM_sig_valmed['Precision_val'] = 0\n",
        "SVM_sig_valmed['auc_val'] = 0\n",
        "\n",
        "\n",
        "SVM_poly_trainmed = SVM_sig_trainmed\n",
        "SVM_rbf_trainmed = SVM_sig_trainmed\n",
        "RT_trainmed = SVM_sig_trainmed\n",
        "\n",
        "SVM_poly_valmed = SVM_sig_valmed\n",
        "SVM_rbf_valmed = SVM_sig_valmed\n",
        "RT_valmed = SVM_sig_valmed\n",
        "\n"
      ],
      "metadata": {
        "id": "TFicYuXGJw-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################################################################\n",
        "############################################-------VALIDACAO CRUZADA-------####################################################################\n",
        "###############################################################################################################################################\n",
        "\n",
        "#%% SEPARA TREINO E TESTE\n",
        "n_k = 10\n",
        "X_train_nk = X_train_n\n",
        "skf = StratifiedKFold(n_splits=n_k, shuffle=True, random_state=0)\n",
        "skf.get_n_splits(X_train_nk, y_train)\n",
        "l = 0\n",
        "for k, (train_index, val_index) in enumerate(skf.split(X_train_nk, y_train)):\n",
        "  l = l + 1\n",
        "  X_train_n, X_val_n = X_train_nk[train_index], X_train_nk[val_index]\n",
        "  y_trainT, y_val_n = y_train[train_index], y_train[val_index]\n",
        "  X_train_n = np.transpose(X_train_n)\n",
        "  X_val_n = np.transpose(X_val_n)\n",
        "\n",
        "    # SEPARA CLASSES\n",
        "  y1_train_n = y_trainT>0\n",
        "  X1_train_n = np.transpose(X_train_n[:,y1_train_n])\n",
        "  X0_train_n = np.transpose(np.delete(X_train_n,y1_train_n,axis=1))\n",
        "  y0_train_n = np.delete(y_trainT,y1_train_n)\n",
        "  y1_train_n = y_trainT[y1_train_n]\n",
        "\n",
        "  # REMOVE OUTLIERS TREINO\n",
        "  nstd_out = 10\n",
        "  N_excl_tr = 0\n",
        "  for i in range(L):\n",
        "      outliers0_train = np.array(main.t3_remoutliers(X0_train_n[:,i], nstd_out))\n",
        "      outliers1_train = np.array(main.t3_remoutliers(X1_train_n[:,i], nstd_out))\n",
        "      if np.size(outliers0_train)>0:\n",
        "          X0_train_n = np.delete(X0_train_n,outliers0_train,axis=0)\n",
        "          y0_train_n = np.delete(y0_train_n,outliers0_train)\n",
        "      if np.size(outliers1_train)>0:\n",
        "          X1_train_n = np.delete(X1_train_n,outliers1_train,axis=0)\n",
        "          y1_train_n = np.delete(y1_train_n,outliers1_train)\n",
        "      N_excl_tr = N_excl_tr + len(outliers0_train) + len(outliers1_train)\n",
        "\n",
        "\n",
        "  # SUBAMOSTRAGEM CLASSE 0\n",
        "  X0_train_n, X0_c, y0_train_n, y0_c = train_test_split(X0_train_n, y0_train_n, test_size=0.9, random_state=0)\n",
        "\n",
        "\n",
        "  # DADOS DE TREINO TOTAL\n",
        "  X_train_n = np.transpose(np.vstack((X0_train_n, X1_train_n)))\n",
        "  y_train_n = np.concatenate((y0_train_n, y1_train_n))\n",
        "  (L,N_train_n) = np.shape(X_train_n)\n",
        "  (L,N_val_n) = np.shape(X_val_n)\n",
        "\n",
        "\n",
        "  # TRANSFORMA ESPACO\n",
        "  Ls_PCA = 55   # mse<5\n",
        "  w,mse,X_train_t,v = main.t4_pca(X_train_n, Ls_PCA)\n",
        "  X_val_t = np.dot(v.T,X_val_n)     # utiliza criterio dos dados de treino\n",
        "\n",
        "  ica = FastICA(n_components=Ls_PCA)\n",
        "  X_train_t = ica.fit_transform(np.transpose(X_train_t))  # estimated independent sources\n",
        "  X_val_t = ica.transform(np.transpose(X_val_t))\n",
        "  X_train_t = np.transpose(X_train_t)\n",
        "  X_val_t = np.transpose(X_val_t)\n",
        "\n",
        "\n",
        "  # SELECAO ESCALAR COM RANDOM FOREST\n",
        "  arvores = RandomForestClassifier(n_estimators=200, min_samples_split=50, min_samples_leaf=20, random_state=0)\n",
        "  arvores.fit(np.transpose(X_train_t), y_train_n)\n",
        "  ordem = arvores.feature_importances_\n",
        "  ordem = np.argsort(-ordem)\n",
        "  Ls_SE = 40    # numero de caracteristicas a selecionar\n",
        "  X_train_s = np.zeros((Ls_SE,N_train_n))\n",
        "  X_val_s = np.zeros((Ls_SE,N_val_n))\n",
        "  for i in range(Ls_SE):\n",
        "      X_train_s[i,:] = X_train_t[ordem[i],:]\n",
        "      X_val_s[i,:] = X_val_t[ordem[i],:]    # utiliza criterio dos dados de treino\n",
        "\n",
        "\n",
        "  # SEPARA CLASSES\n",
        "  y1_train_t = y_train_n>0\n",
        "  X1_train_t = X_train_s[:,y1_train_t]\n",
        "  X0_train_t = np.delete(X_train_s,y1_train_t,axis=1)\n",
        "  y0_train_t = np.delete(y_train_n,y1_train_t)\n",
        "  y1_train_t = y_train_n[y1_train_t]\n",
        "  y1_val_t = y_val_n>0\n",
        "  X1_val_t = X_val_t[:,y1_val_t]\n",
        "  X0_val_t = np.delete(X_val_t,y1_val_t,axis=1)\n",
        "  y0_val_t = np.delete(y_val_n,y1_val_t)\n",
        "  y1_val_t = y_val_n[y1_val_t]\n",
        "\n",
        "\n",
        "  #%% RANDOM FOREST\n",
        "  n_estimators = 1000\n",
        "  min_samples_split = 50\n",
        "  min_samples_leaf = 5\n",
        "  arvores = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=0)\n",
        "  arvores.fit(np.transpose(X_train_s), y_train_n)\n",
        "  y_train_pred = arvores.predict(np.transpose(X_train_s))\n",
        "  y_val_pred = arvores.predict(np.transpose(X_val_s))\n",
        "\n",
        "\n",
        "  # AVALIACAO\n",
        "  param={}\n",
        "  param['outlier'] = nstd_out\n",
        "  param['PCA-Ls'] = Ls_PCA\n",
        "  param['RT_Selec'] = Ls_SE\n",
        "  param['classificador'] = 'Random Forest'\n",
        "  param['n_estimators'] = n_estimators\n",
        "  param['min_samples_split'] = min_samples_split\n",
        "  param['min_samples_leaf'] = min_samples_leaf\n",
        "  train_aval={}\n",
        "  val_aval={}\n",
        "  train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "  val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "  train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "  val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "  c_target=1\n",
        "  train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "  train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "  train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "  train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "  train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "  train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "  train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "  train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "  val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "  val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "  val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "  val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "  val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "  val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "  val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "  val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "  RT_trainmed['acc_train'] = RT_trainmed['acc_train'] + train_aval['acc_train']/n_k\n",
        "  RT_trainmed['confMatrix_train'] = RT_trainmed['confMatrix_train'] + train_aval['confMatrix_train']/n_k\n",
        "  RT_trainmed['TP_train'] = RT_trainmed['TP_train'] + train_aval['TP_train']/n_k\n",
        "  RT_trainmed['FN_train'] = RT_trainmed['FN_train'] + train_aval['FN_train']/n_k\n",
        "  RT_trainmed['TN_train'] = RT_trainmed['TN_train'] + train_aval['TN_train']/n_k\n",
        "  RT_trainmed['FP_train'] = RT_trainmed['FP_train'] + train_aval['FP_train']/n_k\n",
        "  RT_trainmed['Sensibilidade_train'] = RT_trainmed['Sensibilidade_train'] + train_aval['Sensibilidade_train']/n_k\n",
        "  RT_trainmed['Especificidade_train'] = RT_trainmed['Especificidade_train'] + train_aval['Especificidade_train']/n_k\n",
        "  RT_trainmed['Precision_train'] = RT_trainmed['Precision_train'] + train_aval['Precision_train']/n_k\n",
        "  RT_trainmed['auc_train'] = RT_trainmed['auc_train'] + train_aval['auc_train']/n_k\n",
        "  RT_valmed['acc_val'] = RT_valmed['acc_val'] + val_aval['acc_val']/n_k\n",
        "  RT_valmed['confMatrix_val'] = RT_valmed['confMatrix_val'] + val_aval['confMatrix_val']/n_k\n",
        "  RT_valmed['TP_val'] = RT_valmed['TP_val'] + val_aval['TP_val']/n_k\n",
        "  RT_valmed['FN_val'] = RT_valmed['FN_val'] + val_aval['FN_val']/n_k\n",
        "  RT_valmed['TN_val'] = RT_valmed['TN_val'] + val_aval['TN_val']/n_k\n",
        "  RT_valmed['FP_val'] = RT_valmed['FP_val'] + val_aval['FP_val']/n_k\n",
        "  RT_valmed['Sensibilidade_val'] = RT_valmed['Sensibilidade_val'] + val_aval['Sensibilidade_val']/n_k\n",
        "  RT_valmed['Especificidade_val'] = RT_valmed['Especificidade_val'] + val_aval['Especificidade_val']/n_k\n",
        "  RT_valmed['Precision_val'] = RT_valmed['Precision_val'] + val_aval['Precision_val']/n_k\n",
        "  RT_valmed['auc_val'] = RT_valmed['auc_val'] + val_aval['auc_val']/n_k\n",
        "\n",
        "\n",
        "\n",
        "  # EXPORTA RESULTADOS\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in train_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in val_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in param.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "\n",
        "  # SVM RBF\n",
        "  C = 30\n",
        "  kernel = 'rbf'\n",
        "  tol = 0.01\n",
        "  gamma = 0.5\n",
        "  clf = SVC(C=C, kernel=kernel, tol=tol, gamma=gamma)\n",
        "  clf.fit(np.transpose(X_train_s), y_train_n)\n",
        "  y_train_pred = clf.predict(np.transpose(X_train_s))\n",
        "  y_val_pred = clf.predict(np.transpose(X_val_s))\n",
        "\n",
        "\n",
        "  # AVALIACAO\n",
        "  param={}\n",
        "  param['outlier'] = nstd_out\n",
        "  param['PCA-Ls'] = Ls_PCA\n",
        "  param['RT_Selec'] = Ls_SE\n",
        "  param['classificador'] = 'SVM - rbf'\n",
        "  param['C'] = C\n",
        "  param['kernel'] = kernel\n",
        "  param['tol'] = tol\n",
        "  param['gamma'] = gamma\n",
        "  train_aval={}\n",
        "  val_aval={}\n",
        "  train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "  val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "  train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "  val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "  c_target=1\n",
        "  train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "  train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "  train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "  train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "  train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "  train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "  train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "  train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "  val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "  val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "  val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "  val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "  val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "  val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "  val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "  val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "  SVM_rbf_trainmed['acc_train'] = SVM_rbf_trainmed['acc_train'] + train_aval['acc_train']/n_k\n",
        "  SVM_rbf_trainmed['confMatrix_train'] = SVM_rbf_trainmed['confMatrix_train'] + train_aval['confMatrix_train']/n_k\n",
        "  SVM_rbf_trainmed['TP_train'] = SVM_rbf_trainmed['TP_train'] + train_aval['TP_train']/n_k\n",
        "  SVM_rbf_trainmed['FN_train'] = SVM_rbf_trainmed['FN_train'] + train_aval['FN_train']/n_k\n",
        "  SVM_rbf_trainmed['TN_train'] = SVM_rbf_trainmed['TN_train'] + train_aval['TN_train']/n_k\n",
        "  SVM_rbf_trainmed['FP_train'] = SVM_rbf_trainmed['FP_train'] + train_aval['FP_train']/n_k\n",
        "  SVM_rbf_trainmed['Sensibilidade_train'] = SVM_rbf_trainmed['Sensibilidade_train'] + train_aval['Sensibilidade_train']/n_k\n",
        "  SVM_rbf_trainmed['Especificidade_train'] = SVM_rbf_trainmed['Especificidade_train'] + train_aval['Especificidade_train']/n_k\n",
        "  SVM_rbf_trainmed['Precision_train'] = SVM_rbf_trainmed['Precision_train'] + train_aval['Precision_train']/n_k\n",
        "  SVM_rbf_trainmed['auc_train'] = SVM_rbf_trainmed['auc_train'] + train_aval['auc_train']/n_k\n",
        "  SVM_rbf_valmed['acc_val'] = SVM_rbf_valmed['acc_val'] + val_aval['acc_val']/n_k\n",
        "  SVM_rbf_valmed['confMatrix_val'] = SVM_rbf_valmed['confMatrix_val'] + val_aval['confMatrix_val']/n_k\n",
        "  SVM_rbf_valmed['TP_val'] = SVM_rbf_valmed['TP_val'] + val_aval['TP_val']/n_k\n",
        "  SVM_rbf_valmed['FN_val'] = SVM_rbf_valmed['FN_val'] + val_aval['FN_val']/n_k\n",
        "  SVM_rbf_valmed['TN_val'] = SVM_rbf_valmed['TN_val'] + val_aval['TN_val']/n_k\n",
        "  SVM_rbf_valmed['FP_val'] = SVM_rbf_valmed['FP_val'] + val_aval['FP_val']/n_k\n",
        "  SVM_rbf_valmed['Sensibilidade_val'] = SVM_rbf_valmed['Sensibilidade_val'] + val_aval['Sensibilidade_val']/n_k\n",
        "  SVM_rbf_valmed['Especificidade_val'] = SVM_rbf_valmed['Especificidade_val'] + val_aval['Especificidade_val']/n_k\n",
        "  SVM_rbf_valmed['Precision_val'] = SVM_rbf_valmed['Precision_val'] + val_aval['Precision_val']/n_k\n",
        "  SVM_rbf_valmed['auc_val'] = SVM_rbf_valmed['auc_val'] + val_aval['auc_val']/n_k\n",
        "\n",
        "\n",
        "  # EXPORTA RESULTADOS\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in train_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in val_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in param.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "\n",
        "  # SVM POL\n",
        "  C = 20\n",
        "  kernel = 'poly'\n",
        "  degree = 4    #melhor no treino, pior no teste, sobreajuste\n",
        "  coef0 = 2\n",
        "  clf = SVC(C=C, kernel=kernel, degree=degree, coef0=coef0)\n",
        "  clf.fit(np.transpose(X_train_t), y_train_n)\n",
        "  y_train_pred = clf.predict(np.transpose(X_train_t))\n",
        "  y_val_pred = clf.predict(np.transpose(X_val_t))\n",
        "\n",
        "\n",
        "  # AVALIACAO\n",
        "  param={}\n",
        "  param['outlier'] = nstd_out\n",
        "  param['PCA-Ls'] = Ls_PCA\n",
        "  param['RT_Selec'] = Ls_SE\n",
        "  param['classificador'] = 'SVM - ploy'\n",
        "  param['C'] = C\n",
        "  param['kernel'] = kernel\n",
        "  param['degree'] = degree\n",
        "  param['coef0'] = coef0\n",
        "  train_aval={}\n",
        "  val_aval={}\n",
        "  train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "  val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "  train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "  val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "  c_target=1\n",
        "  train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "  train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "  train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "  train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "  train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "  train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "  train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "  train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "  val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "  val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "  val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "  val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "  val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "  val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "  val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "  val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "  SVM_poly_trainmed['acc_train'] = SVM_poly_trainmed['acc_train'] + train_aval['acc_train']/n_k\n",
        "  SVM_poly_trainmed['confMatrix_train'] = SVM_poly_trainmed['confMatrix_train'] + train_aval['confMatrix_train']/n_k\n",
        "  SVM_poly_trainmed['TP_train'] = SVM_poly_trainmed['TP_train'] + train_aval['TP_train']/n_k\n",
        "  SVM_poly_trainmed['FN_train'] = SVM_poly_trainmed['FN_train'] + train_aval['FN_train']/n_k\n",
        "  SVM_poly_trainmed['TN_train'] = SVM_poly_trainmed['TN_train'] + train_aval['TN_train']/n_k\n",
        "  SVM_poly_trainmed['FP_train'] = SVM_poly_trainmed['FP_train'] + train_aval['FP_train']/n_k\n",
        "  SVM_poly_trainmed['Sensibilidade_train'] = SVM_poly_trainmed['Sensibilidade_train'] + train_aval['Sensibilidade_train']/n_k\n",
        "  SVM_poly_trainmed['Especificidade_train'] = SVM_poly_trainmed['Especificidade_train'] + train_aval['Especificidade_train']/n_k\n",
        "  SVM_poly_trainmed['Precision_train'] = SVM_poly_trainmed['Precision_train'] + train_aval['Precision_train']/n_k\n",
        "  SVM_poly_trainmed['auc_train'] = SVM_poly_trainmed['auc_train'] + train_aval['auc_train']/n_k\n",
        "  SVM_poly_valmed['acc_val'] = SVM_poly_valmed['acc_val'] + val_aval['acc_val']/n_k\n",
        "  SVM_poly_valmed['confMatrix_val'] = SVM_poly_valmed['confMatrix_val'] + val_aval['confMatrix_val']/n_k\n",
        "  SVM_poly_valmed['TP_val'] = SVM_poly_valmed['TP_val'] + val_aval['TP_val']/n_k\n",
        "  SVM_poly_valmed['FN_val'] = SVM_poly_valmed['FN_val'] + val_aval['FN_val']/n_k\n",
        "  SVM_poly_valmed['TN_val'] = SVM_poly_valmed['TN_val'] + val_aval['TN_val']/n_k\n",
        "  SVM_poly_valmed['FP_val'] = SVM_poly_valmed['FP_val'] + val_aval['FP_val']/n_k\n",
        "  SVM_poly_valmed['Sensibilidade_val'] = SVM_poly_valmed['Sensibilidade_val'] + val_aval['Sensibilidade_val']/n_k\n",
        "  SVM_poly_valmed['Especificidade_val'] = SVM_poly_valmed['Especificidade_val'] + val_aval['Especificidade_val']/n_k\n",
        "  SVM_poly_valmed['Precision_val'] = SVM_poly_valmed['Precision_val'] + val_aval['Precision_val']/n_k\n",
        "  SVM_poly_valmed['auc_val'] = SVM_poly_valmed['auc_val'] + val_aval['auc_val']/n_k\n",
        "\n",
        "  # EXPORTA RESULTADOS\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in train_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in val_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in param.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "\n",
        "  # SVM SIGMOIDE\n",
        "  C = 30\n",
        "  kernel = 'sigmoid'\n",
        "  coef0 = -1\n",
        "  clf = SVC(C=C, kernel=kernel, coef0=coef0)\n",
        "  clf.fit(np.transpose(X_train_t), y_train_n)\n",
        "  y_train_pred = clf.predict(np.transpose(X_train_t))\n",
        "  y_val_pred = clf.predict(np.transpose(X_val_t))\n",
        "\n",
        "\n",
        "  # AVALIACAO\n",
        "  param={}\n",
        "  param['outlier'] = nstd_out\n",
        "  param['PCA-Ls'] = Ls_PCA\n",
        "  param['RT_Selec'] = Ls_SE\n",
        "  param['classificador'] = 'SVM - sig'\n",
        "  param['C'] = C\n",
        "  param['kernel'] = kernel\n",
        "  param['degree'] = degree\n",
        "  param['coef0'] = coef0\n",
        "  train_aval={}\n",
        "  val_aval={}\n",
        "  train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "  val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "  train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "  val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "  c_target=1\n",
        "  train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "  train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "  train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "  train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "  train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "  train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "  train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "  train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "  val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "  val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "  val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "  val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "  val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "  val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "  val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "  val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "  SVM_sig_trainmed['acc_train'] = SVM_sig_trainmed['acc_train'] + train_aval['acc_train']/n_k\n",
        "  SVM_sig_trainmed['confMatrix_train'] = SVM_sig_trainmed['confMatrix_train'] + train_aval['confMatrix_train']/n_k\n",
        "  SVM_sig_trainmed['TP_train'] = SVM_sig_trainmed['TP_train'] + train_aval['TP_train']/n_k\n",
        "  SVM_sig_trainmed['FN_train'] = SVM_sig_trainmed['FN_train'] + train_aval['FN_train']/n_k\n",
        "  SVM_sig_trainmed['TN_train'] = SVM_sig_trainmed['TN_train'] + train_aval['TN_train']/n_k\n",
        "  SVM_sig_trainmed['FP_train'] = SVM_sig_trainmed['FP_train'] + train_aval['FP_train']/n_k\n",
        "  SVM_sig_trainmed['Sensibilidade_train'] = SVM_sig_trainmed['Sensibilidade_train'] + train_aval['Sensibilidade_train']/n_k\n",
        "  SVM_sig_trainmed['Especificidade_train'] = SVM_sig_trainmed['Especificidade_train'] + train_aval['Especificidade_train']/n_k\n",
        "  SVM_sig_trainmed['Precision_train'] = SVM_sig_trainmed['Precision_train'] + train_aval['Precision_train']/n_k\n",
        "  SVM_sig_trainmed['auc_train'] = SVM_sig_trainmed['auc_train'] + train_aval['auc_train']/n_k\n",
        "  SVM_sig_valmed['acc_val'] = SVM_sig_valmed['acc_val'] + val_aval['acc_val']/n_k\n",
        "  SVM_sig_valmed['confMatrix_val'] = SVM_sig_valmed['confMatrix_val'] + val_aval['confMatrix_val']/n_k\n",
        "  SVM_sig_valmed['TP_val'] = SVM_sig_valmed['TP_val'] + val_aval['TP_val']/n_k\n",
        "  SVM_sig_valmed['FN_val'] = SVM_sig_valmed['FN_val'] + val_aval['FN_val']/n_k\n",
        "  SVM_sig_valmed['TN_val'] = SVM_sig_valmed['TN_val'] + val_aval['TN_val']/n_k\n",
        "  SVM_sig_valmed['FP_val'] = SVM_sig_valmed['FP_val'] + val_aval['FP_val']/n_k\n",
        "  SVM_sig_valmed['Sensibilidade_val'] = SVM_sig_valmed['Sensibilidade_val'] + val_aval['Sensibilidade_val']/n_k\n",
        "  SVM_sig_valmed['Especificidade_val'] = SVM_sig_valmed['Especificidade_val'] + val_aval['Especificidade_val']/n_k\n",
        "  SVM_sig_valmed['Precision_val'] = SVM_sig_valmed['Precision_val'] + val_aval['Precision_val']/n_k\n",
        "  SVM_sig_valmed['auc_val'] = SVM_sig_valmed['auc_val'] + val_aval['auc_val']/n_k\n",
        "\n",
        "  # EXPORTA RESULTADOS\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in train_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in val_aval.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      for key, value in param.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "\n",
        "\n",
        "# EXPORTA RESULTADOS\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/RT_trainmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in RT_trainmed.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/SVM_poly_trainmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in SVM_poly_trainmed.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/SVM_rbf_trainmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in SVM_rbf_trainmed.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/SVM_sig_trainmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in SVM_sig_trainmed.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/RT_valmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in RT_valmed.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/SVM_poly_valmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in SVM_poly_valmed.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/SVM_rbf_valmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in SVM_rbf_valmed.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/SVM_sig_valmed.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in SVM_sig_valmed.items():\n",
        "        writer.writerow([key, value])"
      ],
      "metadata": {
        "id": "yVTbwSHe7vev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qlLYb4vmewY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_n = np.transpose(X_train_n)\n",
        "X_test_n = np.transpose(X_test_n)\n",
        "y_test_n = y_test\n",
        "y_trainT = y_train\n",
        "  # SEPARA CLASSES\n",
        "y1_train_n = y_trainT>0\n",
        "X1_train_n = np.transpose(X_train_n[:,y1_train_n])\n",
        "X0_train_n = np.transpose(np.delete(X_train_n,y1_train_n,axis=1))\n",
        "y0_train_n = np.delete(y_trainT,y1_train_n)\n",
        "y1_train_n = y_trainT[y1_train_n]\n",
        "\n",
        "# REMOVE OUTLIERS TREINO\n",
        "nstd_out = 10\n",
        "N_excl_tr = 0\n",
        "for i in range(L):\n",
        "    outliers0_train = np.array(main.t3_remoutliers(X0_train_n[:,i], nstd_out))\n",
        "    outliers1_train = np.array(main.t3_remoutliers(X1_train_n[:,i], nstd_out))\n",
        "    if np.size(outliers0_train)>0:\n",
        "        X0_train_n = np.delete(X0_train_n,outliers0_train,axis=0)\n",
        "        y0_train_n = np.delete(y0_train_n,outliers0_train)\n",
        "    if np.size(outliers1_train)>0:\n",
        "        X1_train_n = np.delete(X1_train_n,outliers1_train,axis=0)\n",
        "        y1_train_n = np.delete(y1_train_n,outliers1_train)\n",
        "    N_excl_tr = N_excl_tr + len(outliers0_train) + len(outliers1_train)\n",
        "\n",
        "\n",
        "# SUBAMOSTRAGEM CLASSE 0\n",
        "X0_train_n, X0_c, y0_train_n, y0_c = train_test_split(X0_train_n, y0_train_n, test_size=0.9, random_state=0)\n",
        "\n",
        "\n",
        "# DADOS DE TREINO TOTAL\n",
        "X_train_n = np.transpose(np.vstack((X0_train_n, X1_train_n)))\n",
        "y_train_n = np.concatenate((y0_train_n, y1_train_n))\n",
        "(L,N_train_n) = np.shape(X_train_n)\n",
        "(L,N_test_n) = np.shape(X_test_n)\n",
        "\n",
        "\n",
        "# TRANSFORMA ESPACO\n",
        "Ls_PCA = 55   # mse<5\n",
        "w,mse,X_train_t,v = main.t4_pca(X_train_n, Ls_PCA)\n",
        "X_test_t = np.dot(v.T,X_test_n)     # utiliza criterio dos dados de treino\n",
        "\n",
        "ica = FastICA(n_components=Ls_PCA)\n",
        "X_train_t = ica.fit_transform(np.transpose(X_train_t))  # estimated independent sources\n",
        "X_test_t = ica.transform(np.transpose(X_test_t))\n",
        "X_train_t = np.transpose(X_train_t)\n",
        "X_test_t = np.transpose(X_test_t)\n",
        "\n",
        "\n",
        "# SELECAO ESCALAR COM RANDOM FOREST\n",
        "arvores = RandomForestClassifier(n_estimators=200, min_samples_split=50, min_samples_leaf=20, random_state=0)\n",
        "arvores.fit(np.transpose(X_train_t), y_train_n)\n",
        "ordem = arvores.feature_importances_\n",
        "ordem = np.argsort(-ordem)\n",
        "Ls_SE = 40    # numero de caracteristicas a selecionar\n",
        "X_train_s = np.zeros((Ls_SE,N_train_n))\n",
        "X_test_s = np.zeros((Ls_SE,N_test_n))\n",
        "for i in range(Ls_SE):\n",
        "    X_train_s[i,:] = X_train_t[ordem[i],:]\n",
        "    X_test_s[i,:] = X_test_t[ordem[i],:]    # utiliza criterio dos dados de treino\n",
        "\n",
        "\n",
        "# SEPARA CLASSES\n",
        "y1_train_t = y_train_n>0\n",
        "X1_train_t = X_train_s[:,y1_train_t]\n",
        "X0_train_t = np.delete(X_train_s,y1_train_t,axis=1)\n",
        "y0_train_t = np.delete(y_train_n,y1_train_t)\n",
        "y1_train_t = y_train_n[y1_train_t]\n",
        "y1_test_t = y_test_n>0\n",
        "X1_test_t = X_test_t[:,y1_test_t]\n",
        "X0_test_t = np.delete(X_test_t,y1_test_t,axis=1)\n",
        "y0_test_t = np.delete(y_test_n,y1_test_t)\n",
        "y1_test_t = y_test_n[y1_test_t]\n",
        "\n",
        "\n",
        "#%% RANDOM FOREST\n",
        "n_estimators = 1000\n",
        "min_samples_split = 50\n",
        "min_samples_leaf = 5\n",
        "arvores = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=0)\n",
        "arvores.fit(np.transpose(X_train_s), y_train_n)\n",
        "y_train_pred = arvores.predict(np.transpose(X_train_s))\n",
        "y_test_pred = arvores.predict(np.transpose(X_test_s))\n",
        "\n",
        "\n",
        "# AVALIACAO\n",
        "test_atest={}\n",
        "test_atest['acc_test'] = accuracy_score(y_test_n, y_test_pred)\n",
        "test_atest['confMatrix_test'] = confusion_matrix(y_test_n, y_test_pred)\n",
        "c_target=1\n",
        "test_atest['TP_test'] = test_atest['confMatrix_test'][c_target,c_target]\n",
        "test_atest['FN_test'] = np.sum(test_atest['confMatrix_test'][c_target,:]) - test_atest['TP_test']\n",
        "test_atest['TN_test'] = np.sum(test_atest['confMatrix_test'] * np.identity(2)) - test_atest['TP_test']\n",
        "test_atest['FP_test'] = np.sum(test_atest['confMatrix_test']) - test_atest['TP_test'] - test_atest['FN_test'] - test_atest['TN_test']\n",
        "test_atest['Sensibilidade_test'] = test_atest['TP_test']/(test_atest['TP_test']+test_atest['FN_test'])\n",
        "test_atest['Especificidade_test'] = test_atest['TN_test']/(test_atest['TN_test']+test_atest['FP_test'])\n",
        "test_atest['Precision_test'] = test_atest['TP_test']/(test_atest['TP_test']+test_atest['FP_test'])\n",
        "test_atest['auc_test'] = roc_auc_score(y_test_n, y_test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIIcUNcTmEOL",
        "outputId": "6fd90a0e-9872-4d17-bb6a-5c8e483e019d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/Avaliacao Teste.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in test_atest.items():\n",
        "        writer.writerow([key, value])"
      ],
      "metadata": {
        "id": "TQ5Fb_u1qFQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdlA42rlYQbx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# SEPARA TREINO E VALIDA칂츾O\n",
        "X0_train_n, X0_val_n, y0_train_n, y0_val_n = train_test_split(np.transpose(X0_train_n), y0_train_n, test_size=0.1, random_state=0)\n",
        "X1_train_n, X1_val_n, y1_train_n, y1_val_n = train_test_split(np.transpose(X1_train_n), y1_train_n, test_size=0.1, random_state=0)\n",
        "X_train_n = np.transpose(np.vstack((X1_train_n,X0_train_n)))\n",
        "y_train_n = np.concatenate((y1_train_n,y0_train_n))\n",
        "X_val_n = np.transpose(np.vstack((X1_val_n,X0_val_n)))\n",
        "y_val_n = np.concatenate((y1_val_n,y0_val_n))\n",
        "\n",
        "\n",
        "# REMOVE OUTLIERS TREINO\n",
        "nstd_out = 10\n",
        "N_excl_tr = 0\n",
        "for i in range(L):\n",
        "    outliers0_train = np.array(main.t3_remoutliers(X0_train_n[:,i], nstd_out))\n",
        "    outliers1_train = np.array(main.t3_remoutliers(X1_train_n[:,i], nstd_out))\n",
        "    if np.size(outliers0_train)>0:\n",
        "        X0_train_n = np.delete(X0_train_n,outliers0_train,axis=0)\n",
        "        y0_train_n = np.delete(y0_train_n,outliers0_train)\n",
        "    if np.size(outliers1_train)>0:\n",
        "        X1_train_n = np.delete(X1_train_n,outliers1_train,axis=0)\n",
        "        y1_train_n = np.delete(y1_train_n,outliers1_train)\n",
        "    N_excl_tr = N_excl_tr + len(outliers0_train) + len(outliers1_train)\n",
        "\n",
        "\n",
        "# SUBAMOSTRAGEM CLASSE 0\n",
        "X0_train_n, X0_c, y0_train_n, y0_c = train_test_split(X0_train_n, y0_train_n, test_size=0.9, random_state=0)\n",
        "\n",
        "\n",
        "# DADOS DE TREINO TOTAL\n",
        "X_train_n = np.transpose(np.vstack((X0_train_n, X1_train_n)))\n",
        "y_train_n = np.concatenate((y0_train_n, y1_train_n))\n",
        "(L,N_train_n) = np.shape(X_train_n)\n",
        "(L,N_val_n) = np.shape(X_val_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tE3Rrv8E36Q",
        "outputId": "7e3d0cef-6f97-4f78-9fbe-f4aa413668d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# TRANSFORMA ESPACO\n",
        "Ls_PCA = 55   # mse<5\n",
        "w,mse,X_train_t,v = main.t4_pca(X_train_n, Ls_PCA)\n",
        "X_val_t = np.dot(v.T,X_val_n)     # utiliza criterio dos dados de treino\n",
        "\n",
        "ica = FastICA(n_components=Ls_PCA)\n",
        "X_train_t = ica.fit_transform(np.transpose(X_train_t))  # estimated independent sources\n",
        "X_val_t = ica.transform(np.transpose(X_val_t))\n",
        "X_train_t = np.transpose(X_train_t)\n",
        "X_val_t = np.transpose(X_val_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8cZo_x4xe_-"
      },
      "outputs": [],
      "source": [
        "# SELECAO ESCALAR COM RANDOM FOREST\n",
        "arvores = RandomForestClassifier(n_estimators=200, min_samples_split=50, min_samples_leaf=20, random_state=0)\n",
        "arvores.fit(np.transpose(X_train_t), y_train_n)\n",
        "ordem = arvores.feature_importances_\n",
        "ordem = np.argsort(-ordem)\n",
        "Ls_SE = 40    # numero de caracteristicas a selecionar\n",
        "X_train_s = np.zeros((Ls_SE,N_train_n))\n",
        "X_val_s = np.zeros((Ls_SE,N_val_n))\n",
        "for i in range(Ls_SE):\n",
        "    X_train_s[i,:] = X_train_t[ordem[i],:]\n",
        "    X_val_s[i,:] = X_val_t[ordem[i],:]    # utiliza criterio dos dados de treino\n",
        "\n",
        "\n",
        "# SEPARA CLASSES\n",
        "y1_train_t = y_train_n>0\n",
        "X1_train_t = X_train_s[:,y1_train_t]\n",
        "X0_train_t = np.delete(X_train_s,y1_train_t,axis=1)\n",
        "y0_train_t = np.delete(y_train_n,y1_train_t)\n",
        "y1_train_t = y_train_n[y1_train_t]\n",
        "y1_val_t = y_val_n>0\n",
        "X1_val_t = X_val_t[:,y1_val_t]\n",
        "X0_val_t = np.delete(X_val_t,y1_val_t,axis=1)\n",
        "y0_val_t = np.delete(y_val_n,y1_val_t)\n",
        "y1_val_t = y_val_n[y1_val_t]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoN1DrjGX3BX"
      },
      "outputs": [],
      "source": [
        "l=0\n",
        "\n",
        "#%% RANDOM FOREST\n",
        "n_estimators = 1000\n",
        "min_samples_split = 50\n",
        "min_samples_leaf = 5\n",
        "arvores = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=0)\n",
        "arvores.fit(np.transpose(X_train_s), y_train_n)\n",
        "y_train_pred = arvores.predict(np.transpose(X_train_s))\n",
        "y_val_pred = arvores.predict(np.transpose(X_val_s))\n",
        "\n",
        "\n",
        "# AVALIACAO\n",
        "param={}\n",
        "param['outlier'] = nstd_out\n",
        "param['PCA-Ls'] = Ls_PCA\n",
        "param['RT_Selec'] = Ls_SE\n",
        "param['classificador'] = 'Random Forest'\n",
        "param['n_estimators'] = n_estimators\n",
        "param['min_samples_split'] = min_samples_split\n",
        "param['min_samples_leaf'] = min_samples_leaf\n",
        "train_aval={}\n",
        "val_aval={}\n",
        "train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "c_target=1\n",
        "train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "\n",
        "\n",
        "# EXPORTA RESULTADOS\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in train_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in val_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in param.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "\n",
        "# SVM RBF\n",
        "C = 30\n",
        "kernel = 'rbf'\n",
        "tol = 0.01\n",
        "gamma = 0.5\n",
        "clf = SVC(C=C, kernel=kernel, tol=tol, gamma=gamma)\n",
        "clf.fit(np.transpose(X_train_s), y_train_n)\n",
        "y_train_pred = clf.predict(np.transpose(X_train_s))\n",
        "y_val_pred = clf.predict(np.transpose(X_val_s))\n",
        "\n",
        "\n",
        "# AVALIACAO\n",
        "param={}\n",
        "param['outlier'] = nstd_out\n",
        "param['PCA-Ls'] = Ls_PCA\n",
        "param['RT_Selec'] = Ls_SE\n",
        "param['classificador'] = 'SVM - rbf'\n",
        "param['C'] = C\n",
        "param['kernel'] = kernel\n",
        "param['tol'] = tol\n",
        "param['gamma'] = gamma\n",
        "train_aval={}\n",
        "val_aval={}\n",
        "train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "c_target=1\n",
        "train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "\n",
        "\n",
        "# EXPORTA RESULTADOS\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in train_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in val_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in param.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "\n",
        "# SVM POL\n",
        "C = 20\n",
        "kernel = 'poly'\n",
        "degree = 4    #melhor no treino, pior no teste, sobreajuste\n",
        "coef0 = 2\n",
        "clf = SVC(C=C, kernel=kernel, degree=degree, coef0=coef0)\n",
        "clf.fit(np.transpose(X_train_t), y_train_n)\n",
        "y_train_pred = clf.predict(np.transpose(X_train_t))\n",
        "y_val_pred = clf.predict(np.transpose(X_val_t))\n",
        "\n",
        "\n",
        "# AVALIACAO\n",
        "param={}\n",
        "param['outlier'] = nstd_out\n",
        "param['PCA-Ls'] = Ls_PCA\n",
        "param['RT_Selec'] = Ls_SE\n",
        "param['classificador'] = 'SVM - ploy'\n",
        "param['C'] = C\n",
        "param['kernel'] = kernel\n",
        "param['degree'] = degree\n",
        "param['coef0'] = coef0\n",
        "train_aval={}\n",
        "val_aval={}\n",
        "train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "c_target=1\n",
        "train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "\n",
        "# EXPORTA RESULTADOS\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in train_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in val_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in param.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "\n",
        "# SVM SIGMOIDE\n",
        "C = 30\n",
        "kernel = 'sigmoid'\n",
        "coef0 = -1\n",
        "clf = SVC(C=C, kernel=kernel, coef0=coef0)\n",
        "clf.fit(np.transpose(X_train_t), y_train_n)\n",
        "y_train_pred = clf.predict(np.transpose(X_train_t))\n",
        "y_val_pred = clf.predict(np.transpose(X_val_t))\n",
        "\n",
        "\n",
        "# AVALIACAO\n",
        "param={}\n",
        "param['outlier'] = nstd_out\n",
        "param['PCA-Ls'] = Ls_PCA\n",
        "param['RT_Selec'] = Ls_SE\n",
        "param['classificador'] = 'SVM - sig'\n",
        "param['C'] = C\n",
        "param['kernel'] = kernel\n",
        "param['degree'] = degree\n",
        "param['coef0'] = coef0\n",
        "train_aval={}\n",
        "val_aval={}\n",
        "train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "c_target=1\n",
        "train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "\n",
        "\n",
        "# EXPORTA RESULTADOS\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in train_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in val_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in param.items():\n",
        "        writer.writerow([key, value])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIg5PvM860Xu"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "from keras.initializers import Zeros, Ones, glorot_uniform, he_normal\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "opt = SGD(learning_rate=0.001)\n",
        "\n",
        "# REDE NEURAL\n",
        "D1 = Ls_SE\n",
        "D2 = 120\n",
        "D3 = 60\n",
        "D4 = 1\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "batch_size = 50\n",
        "epochs = 1000\n",
        "model = Sequential()\n",
        "#model.add(Dense(units=D1, kernel_initializer=he_normal(), activation='relu', input_dim=D1))\n",
        "model.add(Dense(units=D1, activation='relu', input_dim=D1))\n",
        "model.add(Dense(units=D2, activation='tanh', input_dim=D1))\n",
        "model.add(Dense(units=D3, activation='tanh', input_dim=D2))\n",
        "model.add(Dense(units=D4, activation='softmax', input_dim=D3))\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "early_stopping = EarlyStopping(monitor='accuracy', patience=250, restore_best_weights=True)   # define parada apos estagnacao de acuracia por 250 epocas, retoma melhores pesos\n",
        "model.fit(np.transpose(X_train_s), y_train_n, validation_data=(np.transpose(X_val_s), y_val_n), batch_size=batch_size, epochs=epochs, callbacks=[early_stopping], verbose=1)\n",
        "y_train_pred = model.predict(np.transpose(X_train_s))\n",
        "y_val_pred = model.predict(np.transpose(X_val_s))\n",
        "y_train_pred = (y_train_pred<0.5).astype(int)\n",
        "y_val_pred = (y_val_pred<0.5).astype(int)\n",
        "\n",
        "\n",
        "# AVALIACAO\n",
        "param={}\n",
        "param['outlier'] = nstd_out\n",
        "param['PCA-Ls'] = Ls_PCA\n",
        "param['RT_Selec'] = Ls_SE\n",
        "param['classificador'] = 'Rede Neural'\n",
        "param['D1'] = D1\n",
        "param['D2'] = D2\n",
        "param['D3'] = D3\n",
        "param['D4'] = D4\n",
        "param['epochs'] = epochs\n",
        "param['batch_size'] = batch_size\n",
        "param['optimezer'] = optimizer\n",
        "param['loss'] = loss\n",
        "train_aval={}\n",
        "val_aval={}\n",
        "train_aval['acc_train'] = accuracy_score(y_train_n, y_train_pred)\n",
        "val_aval['acc_val'] = accuracy_score(y_val_n, y_val_pred)\n",
        "train_aval['confMatrix_train'] = confusion_matrix(y_train_n, y_train_pred)\n",
        "val_aval['confMatrix_val'] = confusion_matrix(y_val_n, y_val_pred)\n",
        "c_target=1\n",
        "train_aval['TP_train'] = train_aval['confMatrix_train'][c_target,c_target]\n",
        "train_aval['FN_train'] = np.sum(train_aval['confMatrix_train'][c_target,:]) - train_aval['TP_train']\n",
        "train_aval['TN_train'] = np.sum(train_aval['confMatrix_train'] * np.identity(2)) - train_aval['TP_train']\n",
        "train_aval['FP_train'] = np.sum(train_aval['confMatrix_train']) - train_aval['TP_train'] - train_aval['FN_train'] - train_aval['TN_train']\n",
        "train_aval['Sensibilidade_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FN_train'])\n",
        "train_aval['Especificidade_train'] = train_aval['TN_train']/(train_aval['TN_train']+train_aval['FP_train'])\n",
        "train_aval['Precision_train'] = train_aval['TP_train']/(train_aval['TP_train']+train_aval['FP_train'])\n",
        "train_aval['auc_train'] = roc_auc_score(y_train_n, y_train_pred)\n",
        "val_aval['TP_val'] = val_aval['confMatrix_val'][c_target,c_target]\n",
        "val_aval['FN_val'] = np.sum(val_aval['confMatrix_val'][c_target,:]) - val_aval['TP_val']\n",
        "val_aval['TN_val'] = np.sum(val_aval['confMatrix_val'] * np.identity(2)) - val_aval['TP_val']\n",
        "val_aval['FP_val'] = np.sum(val_aval['confMatrix_val']) - val_aval['TP_val'] - val_aval['FN_val'] - val_aval['TN_val']\n",
        "val_aval['Sensibilidade_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FN_val'])\n",
        "val_aval['Especificidade_val'] = val_aval['TN_val']/(val_aval['TN_val']+val_aval['FP_val'])\n",
        "val_aval['Precision_val'] = val_aval['TP_val']/(val_aval['TP_val']+val_aval['FP_val'])\n",
        "val_aval['auc_val'] = roc_auc_score(y_val_n, y_val_pred)\n",
        "\n",
        "\n",
        "# EXPORTA RESULTADOS\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_train_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in train_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_val_aval.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in val_aval.items():\n",
        "        writer.writerow([key, value])\n",
        "\n",
        "filename = '/content/drive/MyDrive/Po패s/Aprendizado de Ma패quina/Projeto/aval/' + param['classificador']  + str(l) + '_parametros.csv'\n",
        "with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in param.items():\n",
        "        writer.writerow([key, value])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwSRT1_B-axt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2tH6e75Q4Fa8EtG8605xo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}